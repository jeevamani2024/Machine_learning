---
title: "Naive_Bayes_model"
author: "Jeeva Thangamani"
date: "2023-10-15"
output:
  word_document: default
  html_document: default
---
SUMMARY:

1.In situations when an accident has been reported without further details, it is presumed that injuries may have occurred (INJURY = Yes) in order to represent the worst-case scenario, as indicated by MAX_SEV_IR. According to the rules, if MAX_SEV_IR is 1 or 2, there has been an injury of some kind (INJURY = Yes). On the other hand, if MAX_SEV_IR is 0, there isn't any inferred harm (INJURY = No). As a result, unless fresh information to the contrary, it is safe to presume that there was some amount of harm from the accident when there is a lack of supporting facts. 

2.There is a total count of "20721 NO and yes are 21462." To create a new data frame with only 24 records and 3 variables (Injury, Weather, and Traffic), the following steps were taken: 1. A pivot table was constructed with the variables traffic, weather, and injury to arrange the data in a tabular format. 2. The variable Injury was removed from the data frame as it wouldn't be used in the subsequent analysis. For the first 24 entries in the data frame, Bayesian probabilities were estimated to assess the likelihood of injury occurrence. 
Accidents were classified as likely or not likely to result in injuries using a cutoff criterion of 0.5 based on these probabilities. The naive Bayesian conditional probability of harm was calculated by setting WEATHER_R and TRAF_CON_R to 1, resulting in specific outcomes: 
- If there is an injury, the likelihood is 0. 
- If there is no injury, the likelihood is 1. 
The Naive Bayes model's predictions and the actual Bayes classification yielded the following results: [List of "yes" and "no" classifications for the 24 records]. In this classification, records are categorized as either "yes" or "no." Notably, some positions have the same values for both categories, indicating that both classifications agree on the ranking or order of observations. This suggests a common understanding of the data's factors between both classes. 

3.sThe dataset is then divided into a training set (60% of the data) and a validation set (40% of the data). The model is trained using the training data, and the full dataset is employed to evaluate the model's performance in forecasting future accidents. Metrics such as accuracy, precision, recall, and F1-score are used for a comprehensive assessment. Following data frame segmentation, the next step involves normalizing the data to ensure each segment is represented as a single row. This normalization is essential to maintain consistent attribute levels and data types, preventing analytical errors and ensuring accurate and meaningful findings for decision-making. The overall error rate for the validation set is approximately 0.47 when expressed as a decimal, indicating that the Naive Bayes classifier performs reasonably well and accurately on this dataset. The classification model's confusion matrix and statistics are as follows: - Accuracy: The model's accuracy is 0.5, signifying that 50% of predictions are correct. - Sensitivity: The sensitivity (true positive rate or recall) is 0.15635, indicating that the model correctly identifies positive cases (injuries) about 15.635% of the time. - Specificity: Specificity is 0.8708, suggesting that the model correctly identifies negative cases (no injuries) approximately 87.08% of the time. 
In summary, the model performs well overall, though it may not be highly accurate in predicting injuries, particularly in positive injury cases. The Naive Bayes approach, while effective, simplifies the assumption of variable independence. It's essential to consider these findings in the context of specific dataset and objectives.

Question:
The file accidentsFull.csv contains information on 42,183 actual automobile accidents in 2001 in the United States that involved one of three levels of injury: NO INJURY, INJURY, or FATALITY. For each accident, additional information is recorded, such as day of week, weather conditions, and road type. A firm might be interested in developing a system for quickly classifying the severity of an accident based on initial reports and associated data in the system (some of which rely on GPS-assisted reporting).

Our goal here is to predict whether an accident just reported will involve an injury (MAX_SEV_IR = 1 or 2) or will not (MAX_SEV_IR = 0). For this purpose, create a dummy variable called INJURY that takes the value “yes” if MAX_SEV_IR = 1 or 2, and otherwise “no.”

Using the information in this dataset, if an accident has just been reported and no further information is available, what should the prediction be? (INJURY = Yes or No?) Why?
Select the first 24 records in the dataset and look only at the response (INJURY) and the two predictors WEATHER_R and TRAF_CON_R. Create a pivot table that examines INJURY as a function of the two predictors for these 24 records. Use all three variables in the pivot table as rows/columns.
Compute the exact Bayes conditional probabilities of an injury (INJURY = Yes) given the six possible combinations of the predictors.
Classify the 24 accidents using these probabilities and a cutoff of 0.5.
Compute manually the naive Bayes conditional probability of an injury given WEATHER_R = 1 and TRAF_CON_R = 1.
Run a naive Bayes classifier on the 24 records and two predictors. Check the model output to obtain probabilities and classifications for all 24 records. Compare this to the exact Bayes classification. Are the resulting classifications equivalent? Is the ranking (= ordering) of observations equivalent?
Let us now return to the entire dataset. Partition the data into training (60%) and validation (40%). 
Run a naive Bayes classifier on the complete training set with the relevant predictors (and INJURY as the response). Note that all predictors are categorical. Show the confusion matrix.
What is the overall error of the validation set?
Submit a report (pdf file) that includes your conclusions, code, and output. Remember to include the summary / conclusions as the first section in your report..


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(e1071)
library(caret)

```

```{r}
library(class)
```

```{r}
accident_data <- read.csv("C:/Users/jeeva thangamani/Documents/GitHub/64060_-jthangam/Assignment_3/accidentsFull.csv")
head(accident_data)
```

```{r}
str(accident_data)
```

```{r}
accident_data$INJURY = ifelse(accident_data$MAX_SEV_IR>0,"yes","no")

```


1.  Using the information in this dataset, if an accident has just been reported and no further information is available, what should the prediction be? (INJURY = Yes or No?) Why?

```{r}
table(accident_data$INJURY)

```
```{r}
#Converting the variables to factors
# Convert variables to factor
for (i in c(1:dim(accident_data)[2])){
accident_data[,i] <- as.factor(accident_data[,i])
}
head(accident_data,n=24)

```

2.  Select the first 24 records in the dataset and look only at the response (INJURY) and the two predictors WEATHER_R and TRAF_CON_R. Create a pivot table that examines INJURY as a function of the two predictors for these 24 records. Use all three variables in the pivot table as rows/columns.

```{r}
# Create a dataframe with 24 rows
accident_data_24 <- accident_data[1:24,c("INJURY", "WEATHER_R", "TRAF_CON_R")]
dim(accident_data_24)

```

```{r}
#Generate a pivot table from the above dataframe
# Generate a pivot table using ftable function
d1 <- ftable(accident_data_24) #ftable for creating pivot table
d2 <- ftable(accident_data_24[,-1]) #pivot table by dropping the first column
# print the table
d1
```
```{r}
d2
```

2.1  Compute the exact Bayes conditional probabilities of an injury (INJURY = Yes) given the six possible combinations of the predictors.

```{r}
## When INJURY = YES
# INJURY = YES, when WEATHER_R = 1, TRAF_CON_R = 0
P1 <- d1[3,1] / d2[1,1] #INJURY = YES, WEATHER_R = 1, TRAF_CON_R = 0
# Print the data
cat("Probabilty injury=yes when weather=1, traffic=0 is", P1,"\n")
```

```{r}
# When INJURY = YES, when WEATHER_R = 2, TRAF_CON_R = 0
P2 <- d1[4,1] / d2[2,1] #INJURY = YES, WEATHER_R = 2, TRAF_CON_R = 0
# Print the data
cat("Probabilty injury=yes when weather=2, traffic=0 is", P2,"\n")
```

```{r}
#INJURY = YES, when WEATHER_R = 1, TRAF_CON_R = 1
P3 <- d1[3,2] / d2[1,2] #INJURY = YES, WEATHER_R = 1, TRAF_CON_R = 1
# Print the data
cat("Probabilty injury=yes when weather=1, traffic=1 is", P3,"\n")

```

```{r}
# INJURY = YES, when WEATHER_R = 2, TRAF_CON_R = 1
P4 <- d1[4,2] / d2[2,2] #INJURY = YES, WEATHER_R = 2, TRAF_CON_R = 1
# Print the data
cat("Probabilty injury=yes when weather=2, traffic=1 is", P4,"\n")
```

```{r}
# INJURY = YES, when WEATHER_R = 1, TRAF_CON_R = 2
P5 <- d1[3,3] / d2[1,3] #INJURY = YES, WEATHER_R = 1, TRAF_CON_R = 2
# Print the data
cat("Probabilty injury=yes when weather=1, traffic=2 is", P5,"\n")
```

```{r}
# INJURY = YES, when WEATHER_R = 2, TRAF_CON_R = 2
P6 <- d1[4,3] / d2[2,3] #INJURY = YES, WEATHER_R = 2, TRAF_CON_R = 2
# Print the data
cat("Probabilty injury=yes when weather=2, traffic=2 is", P6,"\n")
```

```{r}
# Probabilities when INJURY = Yes
cat("list of probabilities when INJURY = yes", "\n")
```

```{r}
c(P1, P2, P3, P4, P5, P6)
```

```{r}
#Considering Injury = no and getting six possible combinations of the predictors.
## When INJURY = NO
# INJURY = no when WEATHER_R = 1, TRAF_CON_R = 0
n1 <- d1[1,1] / d2[1,1] #INJURY = no, WEATHER_R = 1, TRAF_CON_R = 0
# Print the data
cat("Probabilty ijury=no when weather=1, traffic=0 is", n1,"\n")
```

```{r}
## Probabilty ijury=no when weather=1, traffic=0 is 0.3333333
# INJURY = no when WEATHER_R = 2, TRAF_CON_R = 0
n2 <- d1[2,1] / d2[2,1] #INJURY = no, WEATHER_R = 2, TRAF_CON_R = 0
# Print the data
cat("Probabilty injury=no when weather=2, traffic=0 is", n2,"\n")
```

```{r}
## Probabilty injury=no when weather=2, traffic=0 is 0.8181818
# INJURY = no when WEATHER_R = 1, TRAF_CON_R = 1
n3 <- d1[1,2] / d2[1,2] #INJURY = no, WEATHER_R = 1, TRAF_CON_R = 1
# Print the data
cat("Probabilty injury=no when weather=1, traffic=1 is", n3,"\n")

```

```{r}
## Probabilty injury=no when weather=1, traffic=1 is 1
# INJURY = no when WEATHER_R = 2, TRAF_CON_R = 1
n4 <- d1[2,2] / d2[2,2] #INJURY = no, WEATHER_R = 2, TRAF_CON_R = 1
# Print the data
cat("Probabilty injury=no when weather=2, traffic=1 is", n4,"\n")
```

```{r}
# INJURY = no when WEATHER_R = 1, TRAF_CON_R = 2
n5 <- d1[1,3] / d2[1,3] #INJURY = no, WEATHER_R = 1, TRAF_CON_R = 2
# Print the data
cat("Probabilty injury=no when weather=1, traffic=2 is", n5,"\n")
```

```{r}
# INJURY = no when WEATHER_R = 2, TRAF_CON_R = 2
n6 <- d1[2,3] / d2[2,3] #INJURY = no, WEATHER_R = 2, TRAF_CON_R = 2
# Print the data
cat("Probabilty injury=no when weather=2, traffic=2 is", n6,"\n")

```

```{r}
# Probabilities when INJURY = No
cat("list of probabilities when INJURY = NO", "\n")
```

```{r}
c(n1, n2, n3, n4, n5, n6)
```

2.2  Classify the 24 accidents using these probabilities and a cutoff of 0.5.

```{r}
#Assigning the probabilities to the each of the 24rows.
# Taking the values from 0 to 24
probability.inj <- rep(0,24)
# for loop considering iterations from 1 to 24
for(i in 1:24){
# when weather=1;
if (accident_data_24$WEATHER_R[i] == "1") {
# when Traffic = 0
if (accident_data_24$TRAF_CON_R[i]=="0"){
probability.inj[i] = P1
}
# when Traffic = 1
else if (accident_data_24$TRAF_CON_R[i]=="1") {
probability.inj[i] = P3
}
# when Traffic = 2
else if (accident_data_24$TRAF_CON_R[i]=="2") {
probability.inj[i] = P5
}
}
# when weather = 2
else {
# when Traffic = 0
if (accident_data_24$TRAF_CON_R[i]=="0"){
probability.inj[i] = P2
}
# when Traffic = 1
else if (accident_data_24$TRAF_CON_R[i]=="1") {
probability.inj[i] = P4
}
# when Traffic = 2
else if (accident_data_24$TRAF_CON_R[i]=="2") {
probability.inj[i] = P6
}
}
}
# Inserting the probabilities to the table
accident_data_24$probability.inj <- probability.inj
# print the table
head(accident_data_24)
```

```{r}
# Classifying the 24 accidents by cutoff value 0.5 that means if probability was greater than 0.5 the Injury will be yes
accident_data_24$pred.probability <-
ifelse(accident_data_24$probability.inj>0.5, "yes", "no")
# print the table
head(accident_data_24)
```

2.3  Compute manually the naive Bayes conditional probability of an injury given WEATHER_R = 1 and TRAF_CON_R = 1.

```{r}
# Probability of getting Injured when WEATHER_R = 1
PIW <- (d1[3,1] + d1[3,2] + d1[3,3]) / (d1[3,1] + d1[3,2] + d1[3,3] + d1[4,1] + d1[4,2] + d1[4,3])
PIW
```


```{r}
# Probability of getting Injured when TRAF_CON_R = 1
PIT <- (d1[3,2] + d1[4,2]) / (d1[3,1] + d1[3,2] + d1[3,3] + d1[4,1] + d1[4,2] + d1[4,3])
PIT
```
```{r}
# Probability of getting Injured
PII <- (d1[3,1] + d1[3,2] + d1[3,3] + d1[4,1] + d1[4,2] + d1[4,3])/24
PII
```

```{r}
# Probability of not getting Injured when WEATHER_R = 1
PNW <- (d1[1,1] + d1[1,2] + d1[1,3]) / (d1[1,1] + d1[1,2] + d1[1,3] + d1[2,1] + d1[2,2] + d1[2,3])
PNW
```

```{r}
# Probability of not getting Injured when TRAF_CON_R = 1
PNT <- (d1[1,2] + d1[2,2]) / (d1[1,1] + d1[1,2] + d1[1,3] + d1[2,1] + d1[2,2] + d1[2,3])
PNT
```

```{r}
# Probability of not getting Injured
PNI <- (d1[1,1] + d1[1,2] + d1[1,3] + d1[2,1] + d1[2,2] + d1[2,3])/24
PNI

```

```{r}
# Probability of getting Injured when WEATHER_R = 1 and TRAF_CON_R = 1
PIWT1 <- (PIW * PIT * PII)/ ((PIW * PIT * PII) + (PNW * PNT * PNI))
PIWT1
```

```{r}
cat("The naive Bayes conditional probability of an injury given WEATHER_R = 1 and TRAF_CON_R = 1 is", PIT)
```

2.4  Run a naive Bayes classifier on the 24 records and two predictors. Check the model output to obtain probabilities and classifications for all 24 records. Compare this to the exact Bayes classification. Are the resulting classifications equivalent? Is the ranking (= ordering) of observations equivalent?

```{r}
#Run the naiveBayes model
# Run the naiveBayes model by considering Traffic and weather
nb <- naiveBayes(INJURY ~ TRAF_CON_R + WEATHER_R, data = accident_data_24)
# Predicting the data using naiveBayes model
nbt <- predict(nb, newdata = accident_data_24, type = "raw")
# Adding the newly predicted data to accidents24 dataframe
accident_data_24$nbpred.probability <- nbt[,2] # Transfer the "Yes" nb prediction
new_1 <- train(INJURY ~ TRAF_CON_R + WEATHER_R,
data = accident_data_24, method = "nb")

```

```{r}
predict(new_1, newdata = accident_data_24[,c("INJURY", "WEATHER_R", "TRAF_CON_R")])

```


```{r}
predict(new_1, newdata = accident_data_24[,c("INJURY", "WEATHER_R", "TRAF_CON_R")],
type = "raw")
```

3.  Let us now return to the entire dataset. Partition the data into training (60%) and validation (40%).

```{r}
set.seed(1)
train_data <- sample(row.names(accident_data),0.6*dim(accident_data)[1])
valid_data <- setdiff(row.names(accident_data),train_data)
t.df <- accident_data[train_data,]
v.df <- accident_data[valid_data,]
cat("The size of training data is:",nrow(t.df))

```

```{r}
cat("The size of validation data is:",nrow(v.df))
```

3.1  Run a naive Bayes classifier on the complete training set with the relevant predictors (and INJURY as the response). Note that all predictors are categorical. Show the confusion matrix.

3.2  What is the overall error of the validation set?

```{r}
model_train <- naiveBayes(INJURY ~ WEATHER_R + TRAF_CON_R, data =t.df)
validation_class <- predict(model_train, v.df)
norm.values <- preProcess(t.df[,], method = c("center", "scale"))
```

```{r}
a.norm.df <- predict(norm.values, t.df[, ])
v.norm.df <- predict(norm.values, v.df[, ])
levels(a.norm.df)

```


```{r}
class(a.norm.df$INJURY)
```

```{r}
a.norm.df$INJURY <- as.factor(a.norm.df$INJURY)
class(a.norm.df$INJURY)

```

```{r}
nb_model <- naiveBayes(INJURY ~ WEATHER_R + TRAF_CON_R, data = a.norm.df)
prediction <- predict(nb_model, newdata = v.norm.df)
#Ensure that factor levels in validation dataset match those in training dataset
v.norm.df$INJURY <- factor(v.norm.df$INJURY, levels = levels(a.norm.df$INJURY))
# Show the confusion matrix
confusionMatrix(prediction, v.norm.df$INJURY)

```


```{r}
# Calculate the overall error rate
error_rate <- 1 - sum(prediction == v.norm.df$INJURY) / nrow(v.norm.df)
cat("The overall error of the validation set is :",1 - sum(prediction == v.norm.df$INJURY) / nrow(v.norm.df))
    
```





























